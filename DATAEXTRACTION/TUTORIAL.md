"Data Analysis is a process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making." - Wikipedia.

Why should you want to learn data analysis? 
Data analysis skills open up the door to begin learning machine learning and ETL processes. This is also a great way to practice our communication skills. We are given lots of information and data, and it is our job to effectively condense and communicate that data in simple and easy to understand way.

Lets start by talking about the tools you will need for Data Analysis. 
You first need to decide if you are going to use a Auto-managed closed tool or a programming langauge.

Auto-managed tools are software that you can purchase that does the work for you. Compared to programming languages where you have to write your functions and computations to read and display the data as you desire. For example, I have used Tableu in my Data Analysis courses at University. It allows me to pick CSV, TXT, SQL, or countless other datasets. It automatically will pull and read the data. From there, it provides many tools and options to easily manipulate the data and create charts and data. Within a short amount of time, you can master the tool sets it provides and create complex graphs in minutes.
More popular examples include Qlik Q, Looker, and ZOHO Analytics.

The advantage for Auto-managed closed tools are that it is easy to learn, however, they are not free, open source, and are limited with what you can do with them. You can only use the tools that they give you.

For this project, I am going to be using a programming language. Programming languages are great for data science as they are open source, free, and have unlimited potential, however, they do have a more difficult learning curve.
Common programming languages for data science are Python, R, and Julia.

I haven't heard about Julia until beginning this project, but here is what data scientists have to say about it.
"Julia is a high-level and general-purpose language that can be used to write code that is fast to execute and easy to implement for scientific calculations. The language is designed to keep all the needs of scientific researchers and data scientists to optimize the experimentation and design implementation." - towardsdatascience.com

Often when we hear about data science, the language that is used is R. R is able to deal with advanced statistical methods and have effecient preformance.

So, if there are objectively two better langauges, why would I choose to do this project in Python. 
First, there is a bias. Python was my first programming language. When I am learning new things, I prefer to learn them and understand them in Python. Python is similar to a psudeocode. Once I write a program in Python, I am able to better understand it and apply it in other things. As my goal is to learn about data science, it would be more productive to use a tool I already know to gain experience and knowledge. Learning a new language could add extra complexity that I did not intend for.

The Python libraries we can use include pandas, matplotlib, numpy, seaborn, statsmodels, scipy, alebra, and scikit-learn. We will talk more about libraries as we apply them. Don't worry to much about them now. Just understand that you will need to install/import some libraries.

There are 5 steps to this project, 1. Data Extraction, 2. Data Cleaning, 3. Data Wrangling, 4. Analysis, and 5. Action. You may have to cycle through the steps multiple times to achieve your end results.

1. Data Extraction
This is the process of getting the data. Are we using a database like SQL, do we need a webscraper, are we using a file format like CSV, JSON, or XML, are we using an API, are we purchasing the data from a commercial database, or are we using distributed database? 

2. Data Cleaning 
Once you have retrieved the data, now we need to tidy it up. (If you are using your own data/database, the data is most likely cleansed and understood. This step may be skipped). We need to fill or null missing values and data, using data imputation, fixing incorrect types or values, and identifying outliers and non relevant data.

3. Action
Once you have clean data, you need to reshape or rearrange the data. This is to achieve the best data analysis you can get. This would include hierachical data, handling categorical data, reshaping and transforming structures, indexing data for quick access, and merging combining or joining data.

4. Analysis
Once the data is rearranged, you can now begin to analyze your data. This inlcudes exploration, building statistical models, visualization and representations, correlation vs causation, hypothesis testing, statistical analysis, and reporting.

5. Action. 
This is the last step in data analysis. This is where you decide what you are going to do with the data. For this step, we need to have an open mind. We won't truly know what our outcome will be. In some scenarios, we have have more of an ultimatum scenario where one outcome = one action. However, for this project, we will keep a completly open mind and decide what we will do with our data.

You will need to have an IDE and Python installed. I recommened VScode and the latest version of Python.

The enviorment we are using is Jupyter Notebooks. This is a free installatation. To install this just run pip install jupyterlab.
In Jupyter Lab, we will be using Notebooksai

The advantage to this is being able to easily access visualizations of our data along with our documents.